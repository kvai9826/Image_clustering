# =========================================================
# Smart Duplicate Detection & Chat-Aware Subclustering
# =========================================================
# (CLIP + SentenceTransformer + FAISS version)
# =========================================================

import os
import uuid
import sqlite3
import numpy as np
import pandas as pd
from PIL import Image
import imagehash
import streamlit as st

import torch
from transformers import CLIPProcessor, CLIPModel
from sentence_transformers import SentenceTransformer
import faiss

# -----------------------
# CONFIG
# -----------------------
DB_FILE = "claims_smart.db"
IMAGE_FOLDER = "./images"
EMB_DIM_IMG = 512
TEXT_DIM = 384
IMG_WEIGHT = 0.5
TEXT_WEIGHT = 0.5
TOP_K = 5

os.makedirs(IMAGE_FOLDER, exist_ok=True)
device = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------
# MODEL INITIALIZATION
# -----------------------
clip_model_path = os.path.abspath("./clip_model")
clip = CLIPModel.from_pretrained(clip_model_path, local_files_only=True).to(device)
clip_processor = CLIPProcessor.from_pretrained(clip_model_path, local_files_only=True)
sbert = SentenceTransformer("all-MiniLM-L6-v2", device=device)

# -----------------------
# DATABASE SETUP
# -----------------------
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
c = conn.cursor()

c.execute("""
CREATE TABLE IF NOT EXISTS records (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    unique_image_id TEXT,
    sub_cluster_id TEXT,
    customer_id TEXT,
    order_id TEXT,
    ip_country_code TEXT,
    billing_country_code TEXT,
    shipping_country_code TEXT,
    credit_card_country_code TEXT,
    fast_lane INTEGER,
    isfba INTEGER,
    has_prime INTEGER,
    gl_code TEXT,
    payment_method TEXT,
    issuing_bank TEXT,
    description TEXT,
    chat_text TEXT,
    damage_classification TEXT,
    image_hash TEXT,
    embedding BLOB
)
""")
conn.commit()

# -----------------------
# UTILS
# -----------------------

def generate_unique_image_id():
    """Generates short unique ID for each new main cluster."""
    return f"C-{str(uuid.uuid4())[:6]}"

def save_image_to_disk(image, image_hash):
    path = os.path.join(IMAGE_FOLDER, f"{image_hash}.png")
    image.save(path)
    return path

def get_image_embedding(image):
    inputs = clip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        img_emb = clip.get_image_features(inputs["pixel_values"])
    return img_emb.cpu().numpy().flatten().astype(np.float32)

def get_text_embedding(text):
    if not text:
        return np.zeros((TEXT_DIM,), dtype=np.float32)
    emb = sbert.encode([text], normalize_embeddings=True)[0]
    return emb.astype(np.float32)

def get_combined_embedding(image, description, chat_text, damage_text):
    img_emb = get_image_embedding(image)
    text_emb = get_text_embedding(description + " " + chat_text)
    dmg_emb = get_text_embedding(damage_text)
    text_combined = 0.8 * text_emb + 0.2 * dmg_emb

    img_emb /= (np.linalg.norm(img_emb) + 1e-10)
    text_combined /= (np.linalg.norm(text_combined) + 1e-10)

    combined = np.concatenate([IMG_WEIGHT * img_emb, TEXT_WEIGHT * text_combined])
    combined /= (np.linalg.norm(combined) + 1e-10)
    return combined.astype(np.float32)

# -----------------------
# FAISS SETUP
# -----------------------
COMBINED_DIM = EMB_DIM_IMG + TEXT_DIM

def build_faiss_index():
    c.execute("SELECT id, embedding FROM records WHERE embedding IS NOT NULL")
    rows = c.fetchall()
    index = faiss.IndexFlatIP(COMBINED_DIM)
    ids, vectors = [], []

    for rid, blob in rows:
        vec = np.frombuffer(blob, dtype=np.float32)
        if vec.size == COMBINED_DIM:
            vectors.append(vec)
            ids.append(rid)

    if vectors:
        mat = np.vstack(vectors)
        mat /= np.linalg.norm(mat, axis=1, keepdims=True)
        index.add(mat)

    return index, np.array(ids, dtype=np.int64)

faiss_index, faiss_idmap = build_faiss_index()

def add_vector_to_index(vec, db_id):
    global faiss_index, faiss_idmap
    v = vec / (np.linalg.norm(vec) + 1e-10)
    faiss_index.add(v.reshape(1, -1))
    faiss_idmap = np.append(faiss_idmap, db_id)

def rebuild_index():
    global faiss_index, faiss_idmap
    faiss_index, faiss_idmap = build_faiss_index()

# -----------------------
# DUPLICATE CHECK
# -----------------------
def compute_damage_similarity(a, b):
    if not a or not b:
        return 0.0
    a_emb = get_text_embedding(a)
    b_emb = get_text_embedding(b)
    return float(np.dot(a_emb, b_emb))

def check_duplicates_fast(image, description, chat_text, damage_classification):
    image_hash = str(imagehash.phash(image))

    # Quick hash check
    c.execute("SELECT id, unique_image_id FROM records WHERE image_hash=?", (image_hash,))
    row = c.fetchone()
    if row:
        return ("Exact Duplicate", row[1], row[0], [])

    vec = get_combined_embedding(image, description, chat_text, damage_classification)
    if faiss_idmap.size == 0:
        return ("No Duplicate", None, None, [])

    q = vec.reshape(1, -1)
    D, I = faiss_index.search(q, min(TOP_K, faiss_index.ntotal))

    matches = []
    for sim, idx in zip(D[0], I[0]):
        if idx < 0:
            continue
        db_id = int(faiss_idmap[idx])
        c.execute("SELECT unique_image_id, embedding, damage_classification FROM records WHERE id=?", (db_id,))
        r = c.fetchone()
        if not r:
            continue
        uid, emb_blob, stored_damage = r
        emb_vec = np.frombuffer(emb_blob, dtype=np.float32)
        cos_sim = float(np.dot(vec, emb_vec))
        dmg_sim = compute_damage_similarity(damage_classification, stored_damage)
        matches.append((db_id, uid, cos_sim, dmg_sim, stored_damage))

    if not matches:
        return ("No Duplicate", None, None, [])

    matches.sort(key=lambda x: x[2], reverse=True)
    best = matches[0]

    if best[2] >= 0.8:
        return ("Similar Image", best[1], best[0], matches)
    if best[2] >= 0.7 and best[3] >= 0.7:
        return ("Same Narrative", best[1], best[0], matches)
    return ("No Duplicate", None, None, matches)

# -----------------------
# SUBCLUSTER LOGIC
# -----------------------
def get_chat_similarity(a, b):
    if not a or not b:
        return 0.0
    sa, sb = set(a.lower().split()), set(b.lower().split())
    return len(sa & sb) / len(sa | sb)

def compute_similarity(row_a, row_b, chat_sim):
    t1 = ["billing_country_code", "shipping_country_code", "credit_card_country_code"]
    t2 = ["isfba", "has_prime", "fast_lane"]
    t3 = ["ip_country_code", "payment_method", "issuing_bank"]
    s1 = sum(row_a.get(f) == row_b.get(f) for f in t1) / len(t1)
    s2 = sum(row_a.get(f) == row_b.get(f) for f in t2) / len(t2)
    s3 = sum(row_a.get(f) == row_b.get(f) for f in t3) / len(t3)
    return 0.45 * s1 + 0.25 * s2 + 0.15 * s3 + 0.15 * chat_sim

def assign_subcluster(df, new_row):
    subset = df[df["unique_image_id"] == new_row["unique_image_id"]]
    if subset.empty:
        return f"{new_row['unique_image_id']}_S0"

    best_sim, best_id = 0, None
    for _, r in subset.iterrows():
        chat_sim = get_chat_similarity(r.get("chat_text", ""), new_row.get("chat_text", ""))
        sim = compute_similarity(r, new_row, chat_sim)
        if sim > best_sim:
            best_sim, best_id = sim, r["sub_cluster_id"]

    if best_sim >= 0.8:
        return best_id
    num = len(subset["sub_cluster_id"].dropna().unique())
    return f"{new_row['unique_image_id']}_S{num}"

# -----------------------
# STREAMLIT UI
# -----------------------
st.set_page_config(layout="wide")
st.sidebar.title("Navigation")
menu = st.sidebar.radio("Go to:", ["Submit Claim", "Database Viewer", "Rebuild Index"])
st.title("Smart Duplicate Detection & Chat-Aware Subclustering")

if menu == "Rebuild Index":
    st.info("Rebuilding FAISS index...")
    rebuild_index()
    st.success("Index rebuilt successfully.")
    st.stop()

# -------------------------------------------------------
# SUBMIT CLAIM
# -------------------------------------------------------
if menu == "Submit Claim":
    st.subheader("Submit New Claim (Smart Mode)")

    col1, col2 = st.columns(2)
    cust_id = col1.text_input("Customer ID")
    order_id = col2.text_input("Order ID")

    col3, col4 = st.columns(2)
    ip_code = col3.text_input("IP Country Code")
    billing_code = col4.text_input("Billing Country Code")

    col5, col6 = st.columns(2)
    shipping_code = col5.text_input("Shipping Country Code")
    credit_card_code = col6.text_input("Credit Card Country Code")

    col7, col8, col9 = st.columns(3)
    fast_lane = col7.selectbox("Fast Lane", [0, 1])
    isfba = col8.selectbox("Is FBA", [0, 1])
    has_prime = col9.selectbox("Has Prime", [0, 1])

    col10, col11, col12 = st.columns(3)
    gl_code = col10.text_input("GL Code")
    payment_method = col11.text_input("Payment Method")
    issuing_bank = col12.text_input("Issuing Bank")

    description = st.text_area("Image Description", height=80)
    chat_text = st.text_area("Chat Conversation (optional)", height=120)
    damage_classification = st.selectbox(
        "Damage Classification",
        ["Burnt", "Spilled", "Broken", "Scratched", "Missing item",
         "Malfunctioning", "Stained", "Packaging Damaged", "Expired", "Leaking", "Other"]
    )

    uploaded = st.file_uploader("Upload Image", type=["png", "jpg", "jpeg"])

    if uploaded and st.button("Check for Duplicates & Save"):
        img = Image.open(uploaded).convert("RGB")
        status, matched_uid, matched_dbid, top_matches = check_duplicates_fast(img, description, chat_text, damage_classification)
        image_hash = str(imagehash.phash(img))
        save_image_to_disk(img, image_hash)
        emb_vec = get_combined_embedding(img, description, chat_text, damage_classification)
        emb_blob = emb_vec.tobytes()

        uid = generate_unique_image_id() if status == "No Duplicate" else matched_uid
        df_existing = pd.read_sql_query("SELECT * FROM records", conn)

        new_row_meta = {
            "unique_image_id": uid,
            "ip_country_code": ip_code,
            "billing_country_code": billing_code,
            "shipping_country_code": shipping_code,
            "credit_card_country_code": credit_card_code,
            "fast_lane": fast_lane,
            "isfba": isfba,
            "has_prime": has_prime,
            "payment_method": payment_method,
            "issuing_bank": issuing_bank,
            "chat_text": chat_text
        }
        sub_cluster_id = assign_subcluster(df_existing, new_row_meta)

        record = {
            "unique_image_id": uid,
            "sub_cluster_id": sub_cluster_id,
            "customer_id": cust_id,
            "order_id": order_id,
            "ip_country_code": ip_code,
            "billing_country_code": billing_code,
            "shipping_country_code": shipping_code,
            "credit_card_country_code": credit_card_code,
            "fast_lane": fast_lane,
            "isfba": isfba,
            "has_prime": has_prime,
            "gl_code": gl_code,
            "payment_method": payment_method,
            "issuing_bank": issuing_bank,
            "description": description,
            "chat_text": chat_text,
            "damage_classification": damage_classification,
            "image_hash": image_hash,
            "embedding": emb_blob
        }

        cols = list(record.keys())
        vals = [record[c] for c in cols]
        c.execute(f"INSERT INTO records ({','.join(cols)}) VALUES ({','.join(['?']*len(cols))})", vals)
        conn.commit()
        new_id = c.lastrowid
        add_vector_to_index(emb_vec, new_id)

        if status == "No Duplicate":
            st.success(f"✅ New main cluster created: {uid}")
        else:
            st.warning(f"⚠️ {status} found. Linked to cluster: {uid}")
        st.info(f"Assigned Subcluster: {sub_cluster_id}")

# -------------------------------------------------------
# DATABASE VIEWER
# -------------------------------------------------------
elif menu == "Database Viewer":
    st.subheader("Database Viewer — All Inputs + Images")
    df = pd.read_sql_query("""
        SELECT id, unique_image_id, sub_cluster_id, customer_id, order_id,
        ip_country_code, billing_country_code, shipping_country_code,
        credit_card_country_code, fast_lane, isfba, has_prime,
        gl_code, payment_method, issuing_bank, description,
        damage_classification, chat_text, image_hash
        FROM records
    """, conn)

    if df.empty:
        st.info("No records found.")
    else:
        query_col, toggle_col, dl_col = st.columns([3, 1, 1])
        query = query_col.text_input("Search (any field or chat)")
        show_img = toggle_col.toggle("Show Images", value=True)

        if query:
            df = df[df.apply(lambda r: query.lower() in str(r.values).lower(), axis=1)]

        csv = df.to_csv(index=False).encode("utf-8")
        dl_col.download_button("Download CSV", data=csv, file_name="claims_full_audit.csv")

        for uid in df["unique_image_id"].unique():
            st.markdown(f"### Main Cluster: `{uid}`")
            sub_df = df[df["unique_image_id"] == uid]
            for sc in sub_df["sub_cluster_id"].unique():
                st.caption(f"Subcluster: {sc}")
                sub = sub_df[sub_df["sub_cluster_id"] == sc]
                col1, col2 = st.columns([6, 1.2])
                with col1:
                    view = sub.rename(columns={
                        "customer_id": "Customer",
                        "order_id": "Order",
                        "ip_country_code": "IP",
                        "billing_country_code": "Billing",
                        "shipping_country_code": "Shipping",
                        "credit_card_country_code": "Card",
                        "gl_code": "GL",
                        "payment_method": "PayMethod",
                        "issuing_bank": "Bank",
                        "description": "Description",
                        "damage_classification": "Damage",
                        "chat_text": "Chat"
                    })[
                        ["Customer", "Order", "IP", "Billing", "Shipping", "Card", "GL", "PayMethod", "Bank", "Description", "Damage", "Chat"]
                    ]
                    st.dataframe(view, use_container_width=True, hide_index=True)
                with col2:
                    if show_img:
                        for _, r in sub.iterrows():
                            path = os.path.join(IMAGE_FOLDER, f"{r['image_hash']}.png")
                            if os.path.exists(path):
                                st.image(path, width=80)
                            else:
                                st.write("No Image")
                st.markdown("---")